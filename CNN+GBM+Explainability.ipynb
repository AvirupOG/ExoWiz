{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OBvk523uIgS"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip -q install lightgbm xgboost shap scikit-learn matplotlib numpy pandas tqdm\n",
        "!pip -q install captum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data (features + raw series)**"
      ],
      "metadata": {
        "id": "dhYjArdawHCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "VET_PARQUET  = \"/content/vetting_kepler.parquet\"\n",
        "CAND_PARQUET = \"/content/candidates_bls.parquet\"\n",
        "FEAT_PARQUET = \"/content/candidate_features.parquet\"\n",
        "\n",
        "vet_df  = pd.read_parquet(VET_PARQUET)\n",
        "cand_df = pd.read_parquet(CAND_PARQUET)\n",
        "feat_df = pd.read_parquet(FEAT_PARQUET)\n",
        "\n",
        "print(vet_df.shape, cand_df.shape, feat_df.shape)"
      ],
      "metadata": {
        "id": "DRn5b8MVuS1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _to_arr(x):\n",
        "    if isinstance(x, np.ndarray): return x\n",
        "    if isinstance(x, list): return np.asarray(x)\n",
        "    if isinstance(x, pd.Series):\n",
        "        v = next((v for v in x if isinstance(v, (list, np.ndarray))), None)\n",
        "        return np.asarray(v) if v is not None else np.asarray(x)\n",
        "    return np.asarray(x)\n",
        "\n",
        "def phase_fold(time, period, epoch):\n",
        "    t = _to_arr(time).astype(float)\n",
        "    P = float(period); t0 = float(epoch)\n",
        "    ph = ((t - t0 + 0.5*P) % P) / P - 0.5\n",
        "    return ph\n",
        "\n",
        "def bin_folded(phase, flux, nbins=200, robust=True):\n",
        "    ph = phase.copy()\n",
        "    f  = flux.copy()\n",
        "    edges = np.linspace(-0.5, 0.5, nbins+1)\n",
        "    idx = np.digitize(ph, edges) - 1\n",
        "    idx = np.clip(idx, 0, nbins-1)\n",
        "    b = np.empty(nbins); b[:] = np.nan\n",
        "    for i in range(nbins):\n",
        "        sel = (idx == i)\n",
        "        if not np.any(sel): continue\n",
        "        vals = f[sel]\n",
        "        if robust:\n",
        "            b[i] = np.nanmedian(vals)\n",
        "        else:\n",
        "            b[i] = np.nanmean(vals)\n",
        "    if np.isnan(b).any():\n",
        "        good = np.isfinite(b)\n",
        "        if np.any(good):\n",
        "            b[np.isnan(b)] = np.interp(np.flatnonzero(np.isnan(b)), np.flatnonzero(good), b[good])\n",
        "        else:\n",
        "            b[:] = 0.0\n",
        "    return b\n",
        "\n",
        "def make_cnn_views(time, flux, period, epoch, duration, nbins_global=200, nbins_local=100, local_half_width_factor=3.0):\n",
        "    t = _to_arr(time).astype(float)\n",
        "    f = _to_arr(flux).astype(float)\n",
        "    m = np.isfinite(t) & np.isfinite(f)\n",
        "    t, f = t[m], f[m]\n",
        "    if t.size < 100:\n",
        "        return None, None\n",
        "    order = np.argsort(t); t, f = t[order], f[order]\n",
        "    f = f - np.nanmedian(f)\n",
        "\n",
        "    ph = phase_fold(t, period, epoch)\n",
        "    global_view = bin_folded(ph, f, nbins=nbins_global, robust=True)\n",
        "    half = local_half_width_factor * (duration / period)\n",
        "    half = float(np.clip(half, 1e-3, 0.25))\n",
        "    sel = (ph >= -half) & (ph <= half)\n",
        "    if not np.any(sel):\n",
        "        sel = np.argsort(np.abs(ph))[:max(50, nbins_local)]\n",
        "    local_view = bin_folded(ph[sel] / (2*half), f[sel], nbins=nbins_local, robust=True)  # map window to [-0.5,0.5)\n",
        "\n",
        "    def z(x):\n",
        "        mu, sd = np.nanmean(x), np.nanstd(x)\n",
        "        return (x - mu) / (sd if sd > 0 else 1.0)\n",
        "    return z(global_view).astype(np.float32), z(local_view).astype(np.float32)"
      ],
      "metadata": {
        "id": "M491NIIrucdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vet_idx = vet_df.set_index(\"obs_block_id\", drop=False)\n",
        "\n",
        "Xg, Xl, y, groups, ids = [], [], [], [], []\n",
        "drop_count = 0\n",
        "\n",
        "for row in tqdm(feat_df.itertuples(index=False), total=len(feat_df), desc=\"Prep CNN inputs\"):\n",
        "    obid = row.obs_block_id\n",
        "    if obid not in vet_idx.index:\n",
        "        drop_count += 1\n",
        "        continue\n",
        "    block = vet_idx.loc[obid]\n",
        "    if isinstance(block, pd.DataFrame): block = block.iloc[0]\n",
        "\n",
        "    time = block[\"time\"]; flux = block[\"flux\"]\n",
        "    g, l = make_cnn_views(time, flux, row.period, row.epoch, row.duration,\n",
        "                          nbins_global=200, nbins_local=100, local_half_width_factor=3.0)\n",
        "    if g is None:\n",
        "        drop_count += 1\n",
        "        continue\n",
        "\n",
        "    Xg.append(g); Xl.append(l)\n",
        "    y.append(int(0 if pd.isna(row.label) else row.label))\n",
        "    groups.append(block.get(\"target_id\"))\n",
        "    ids.append(obid)\n",
        "\n",
        "Xg = np.stack(Xg)\n",
        "Xl = np.stack(Xl)\n",
        "y  = np.asarray(y)\n",
        "groups = np.asarray(groups)\n",
        "ids = np.asarray(ids)\n",
        "\n",
        "print(\"CNN usable:\", Xg.shape, Xl.shape, y.shape, \"dropped:\", drop_count)"
      ],
      "metadata": {
        "id": "xwotW1bIu0Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/val split**"
      ],
      "metadata": {
        "id": "ptyJYJCmwRVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
        "(train_idx, val_idx), = gss.split(Xg, y, groups=groups)\n",
        "\n",
        "Xg_tr, Xg_va = Xg[train_idx], Xg[val_idx]\n",
        "Xl_tr, Xl_va = Xl[train_idx], Xl[val_idx]\n",
        "y_tr,  y_va  = y[train_idx],  y[val_idx]\n",
        "print(Xg_tr.shape, Xg_va.shape, y_tr.mean(), y_va.mean())"
      ],
      "metadata": {
        "id": "_5m0ONr_vJX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN model (PyTorch) + training**"
      ],
      "metadata": {
        "id": "P6NRFMojwXa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Dual1DCNN(nn.Module):\n",
        "    def __init__(self, in_g=200, in_l=100):\n",
        "        super().__init__()\n",
        "        self.g = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.Conv1d(16, 32, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.l = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.Conv1d(16, 32, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(64+64, 64), nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, xg, xl):\n",
        "        # x: [B, L] -> [B, 1, L]\n",
        "        xg = xg.unsqueeze(1)\n",
        "        xl = xl.unsqueeze(1)\n",
        "        g = self.g(xg).squeeze(-1)  # [B,64]\n",
        "        l = self.l(xl).squeeze(-1)  # [B,64]\n",
        "        h = torch.cat([g, l], dim=1)\n",
        "        logit = self.head(h).squeeze(-1)\n",
        "        return logit\n",
        "\n",
        "tr_ds = TensorDataset(torch.tensor(Xg_tr), torch.tensor(Xl_tr), torch.tensor(y_tr, dtype=torch.float32))\n",
        "va_ds = TensorDataset(torch.tensor(Xg_va), torch.tensor(Xl_va), torch.tensor(y_va, dtype=torch.float32))\n",
        "tr_dl = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "va_dl = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "model = Dual1DCNN().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([max(1.0, (1.0 - y_tr.mean())/(y_tr.mean()+1e-6))], device=device))\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for xg, xl, yb in loader:\n",
        "            xg, xl = xg.to(device), xl.to(device)\n",
        "            logit = model(xg, xl)\n",
        "            prob = torch.sigmoid(logit).cpu().numpy()\n",
        "            ys.append(yb.numpy()); ps.append(prob)\n",
        "    y_true = np.concatenate(ys); y_prob = np.concatenate(ps)\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    return roc_auc_score(y_true, y_prob), average_precision_score(y_true, y_prob), y_prob\n",
        "\n",
        "best_auc, best_state = 0, None\n",
        "for epoch in range(25):\n",
        "    model.train()\n",
        "    for xg, xl, yb in tr_dl:\n",
        "        xg, xl, yb = xg.to(device), xl.to(device), yb.to(device)\n",
        "        logit = model(xg, xl)\n",
        "        loss = criterion(logit, yb)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    auc, ap, yprob_va = evaluate(model, va_dl)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_state = auc, {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "    print(f\"Epoch {epoch+1:02d}  val AUC={auc:.4f}  PR-AUC={ap:.4f}\")\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "cnn_val_prob = evaluate(model, va_dl)[2]\n",
        "print(\"Best CNN val AUC:\", best_auc)"
      ],
      "metadata": {
        "id": "1Q7ZPTXcvN-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GBM on tabular features**"
      ],
      "metadata": {
        "id": "c_ve50mVwb_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_cols = [c for c in feat_df.columns if c not in [\n",
        "    \"obs_block_id\",\"target_id\",\"mission\",\"sector\",\"quarter\",\"campaign\",\"epoch\"\n",
        "] and (pd.api.types.is_numeric_dtype(feat_df[c]) or feat_df[c].dtype==object)]\n",
        "\n",
        "X_full = feat_df[numeric_cols].copy()\n",
        "y_full = feat_df[\"label\"].fillna(0).astype(int).values\n",
        "groups_full = feat_df[\"target_id\"].values\n",
        "\n",
        "ids_full = feat_df[\"obs_block_id\"].values\n",
        "val_mask = np.isin(ids_full, ids[val_idx])\n",
        "train_mask = ~val_mask\n",
        "\n",
        "X_tr, X_va = X_full[train_mask], X_full[val_mask]\n",
        "y_tr2, y_va2 = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "lgb_tr = lgb.Dataset(X_tr, label=y_tr2)\n",
        "lgb_va = lgb.Dataset(X_va, label=y_va2, reference=lgb_tr)\n",
        "\n",
        "params = dict(\n",
        "    objective=\"binary\",\n",
        "    metric=[\"auc\",\"average_precision\"],\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=63,\n",
        "    min_data_in_leaf=30,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    lambda_l2=1.0,\n",
        "    verbose=-1\n",
        ")\n",
        "gbm = lgb.train(params, lgb_tr, num_boost_round=200, valid_sets=[lgb_tr, lgb_va],\n",
        "                valid_names=[\"train\",\"valid\"], early_stopping_rounds=30, verbose_eval=25)\n",
        "\n",
        "gbm_val_prob = gbm.predict(X_va, num_iteration=gbm.best_iteration)\n",
        "print(\"GBM val AUC:\", roc_auc_score(y_va2, gbm_val_prob), \"PR-AUC:\", average_precision_score(y_va2, gbm_val_prob))"
      ],
      "metadata": {
        "id": "Geh0GJtQvXAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Late-fusion**"
      ],
      "metadata": {
        "id": "S12tIij8wsyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "assert np.all(np.isin(ids[val_idx], ids_full[val_mask]))\n",
        "y_val = y_va2\n",
        "\n",
        "stack_X_tr = np.c_[cnn_val_prob, gbm_val_prob]\n",
        "stacker = LogisticRegression(max_iter=1000)\n",
        "stacker.fit(stack_X_tr, y_val)\n",
        "final_val_prob = stacker.predict_proba(stack_X_tr)[:,1]\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "print(\"Fusion val AUC:\", roc_auc_score(y_val, final_val_prob), \"PR-AUC:\", average_precision_score(y_val, final_val_prob))\n",
        "print(\"Fusion weights [cnn, gbm], bias:\", np.r_[stacker.coef_[0], stacker.intercept_])\n"
      ],
      "metadata": {
        "id": "oU08maPBwiHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretability\n",
        "**CNN saliency**"
      ],
      "metadata": {
        "id": "6-P7uF4jwwb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "def forward_local(xl_tensor):\n",
        "    xg_template = torch.tensor(np.mean(Xg_tr, axis=0), dtype=torch.float32, device=device).unsqueeze(0).repeat(xl_tensor.size(0),1)\n",
        "    logit = model(xg_template, xl_tensor)\n",
        "    return torch.sigmoid(logit)\n",
        "\n",
        "ig = IntegratedGradients(forward_local)\n",
        "\n",
        "def plot_local_saliency(sample_idx_in_val):\n",
        "    xl = torch.tensor(Xl_va[sample_idx_in_val], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    attributions, delta = ig.attribute(xl, baselines=torch.zeros_like(xl), target=None, return_convergence_delta=True)\n",
        "    att = attributions.squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.plot(Xl_va[sample_idx_in_val], label=\"local input\")\n",
        "    plt.twinx()\n",
        "    plt.plot(att, alpha=0.7, label=\"saliency\")\n",
        "    plt.title(f\"Local saliency (val idx {sample_idx_in_val})\")\n",
        "    plt.show()\n",
        "\n",
        "plot_local_saliency(0)"
      ],
      "metadata": {
        "id": "giLnh4fJxGHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GBM SHAP**"
      ],
      "metadata": {
        "id": "sbIsJUELxPFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "explainer = shap.TreeExplainer(gbm, feature_perturbation=\"tree_path_dependent\")\n",
        "shap_vals = explainer.shap_values(X_va, check_additivity=False)\n",
        "shap.summary_plot(shap_vals, X_va, feature_names=X_full.columns.tolist(), max_display=20)\n",
        "i = 0\n",
        "shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_vals[i], feature_names=X_full.columns.tolist(), max_display=20)"
      ],
      "metadata": {
        "id": "aWHOtPiSxVCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, os, json, torch\n",
        "\n",
        "os.makedirs(\"/content/models\", exist_ok=True)\n",
        "joblib.dump(gbm, \"/content/models/gbm_lgb.pkl\")\n",
        "joblib.dump(stacker, \"/content/models/stacker_logreg.pkl\")\n",
        "torch.save(model.state_dict(), \"/content/models/cnn_dual.pt\")\n",
        "meta = {\n",
        "    \"cnn_input_shapes\": {\"global\": int(Xg.shape[1]), \"local\": int(Xl.shape[1])},\n",
        "    \"stack_features\": [\"cnn_prob\", \"gbm_prob\"]\n",
        "}\n",
        "json.dump(meta, open(\"/content/models/meta.json\", \"w\"))\n",
        "print(\"Saved models to /content/models\")"
      ],
      "metadata": {
        "id": "w7jl7Q2Axdm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}